

import openai
import os
from prompt_toolkit import PromptSession
import tiktoken

openai.api_key = os.environ["OPENAI_API_KEY"]

# Set the model name to "text-davinci-002" for GPT-3 or "text-davinci-002" for GPT-3.5-turbo.
model_name = "gpt-3.5-turbo"
MAX_TOKENS = 2000
MAX_TOKEN_HIGH_WATER_MARK = MAX_TOKENS - 500

# Start a PromptSession
session = PromptSession()

messages = []
num_tokens_used = 0

# Get the token encoding method for ChatGPT so we can calculate the number of tokens used
def num_tokens_from_string(string: str, language_model) -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.encoding_for_model(language_model)
    num_tokens = len(encoding.encode(string))
    return num_tokens

# Define a function to get completion from the OpenAI API
def complete(message_history):
    # Call the OpenAI API with the given prompt and specified model

    response = openai.ChatCompletion.create(
        model=model_name,
        messages= message_history,
        max_tokens = MAX_TOKENS,
    )

    message_history.append(response["choices"][0]['message'])

    # Return the completed text generated by the API
    return response["choices"][0]['message']['content']

# Start the interactive loop
while True:
    try:
        # Get input from the user using the prompt-toolkit PromptSession
        inp = session.prompt("> ")

        messages.append({"role": "user", "content": str(inp)})     # Append the user's input to the context

        print()
        # Add some formatting to distinguish user input from generated output
        print("\x1b[32mUser: {}\x1b[0m".format(inp))
        print()

        # Call the OpenAI API to generate completions based on the user input
        completions = complete(messages)

        # Print the generated completions
        print(completions)
        print()


    except KeyboardInterrupt:
        # If the user hits Ctrl+C, exit the interactive loop gracefully
        break

    for message in messages:
        num_tokens_used += num_tokens_from_string(message['content'], model_name)

    if num_tokens_used > MAX_TOKEN_HIGH_WATER_MARK:
        #remove more messages than are added to the conversation
        messages.pop(3) # Remove context just after pre-prompts
        messages.pop(4) # Remove context just after pre-prompts
        messages.pop(5) # Remove context just after pre-prompts

